{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n",
      "Eager execution: True\n",
      "Keras version: 2.2.4-tf\n",
      "Cuda version: 10.1\n",
      "Cudnn version: 7\n",
      "Num Physical GPUs Available:  0\n",
      "Num Logical GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, MaxPool2D, Activation\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import get_custom_objects\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))\n",
    "print(\"Cuda version: {}\".format(tf_build_info.cuda_version_number))\n",
    "print(\"Cudnn version: {}\".format(tf_build_info.cudnn_version_number))\n",
    "print(\"Num Physical GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num Logical GPUs Available: \", len(tf.config.experimental.list_logical_devices('GPU')))\n",
    "\n",
    "\n",
    "NETWORK_W          = 416\n",
    "NETWORK_H          = 416\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# needs to be defined as activation class otherwise error\n",
    "# AttributeError: 'Activation' object has no attribute '__name__'    \n",
    "class Mish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'mish'\n",
    "\n",
    "\n",
    "def mysoftplus(x):\n",
    "\n",
    "    mask_min = tf.cast((x<-20.0),tf.float32)\n",
    "    ymin = mask_min*tf.math.exp(x)\n",
    "\n",
    "    mask_max = tf.cast((x>20.0),tf.float32)\n",
    "    ymax = mask_max*x\n",
    "    \n",
    "    mask= tf.cast((abs(x)<=20.0),tf.float32)\n",
    "    y = mask*tf.math.log(tf.math.exp(x) + 1.0)\n",
    "    \n",
    "    return(ymin+ymax+y)    \n",
    "        \n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return (x* tf.math.tanh(mysoftplus(x)))\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})\n",
    "\n",
    "def _conv_block(inp, convs, skip=False):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)), name='zerop_' + str(conv['layer_idx']))(x)  # peculiar padding as darknet prefer left and top\n",
    "        \n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='convn_' + str(conv['layer_idx']) if conv['bnorm'] else 'conv_' + str(conv['layer_idx']),\n",
    "                   use_bias=True)(x)\n",
    "        \n",
    "        if conv['bnorm']: x = BatchNormalization(name='BN_' + str(conv['layer_idx']))(x)    \n",
    "        \n",
    "        if conv['activ'] == 1: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "        if conv['activ'] == 2: x = Activation('mish', name='mish_' + str(conv['layer_idx']))(x) \n",
    "            \n",
    "    return add([skip_connection, x],  name='add_' + str(conv['layer_idx']+1)) if skip else x\n",
    "\n",
    "def make_yolov3_tiny_model():\n",
    "        \n",
    "    input_image = Input(shape=(NETWORK_H, NETWORK_W, 3), name='input_0')\n",
    "\n",
    "    # Layer  0\n",
    "    x = _conv_block(input_image, [{'filter': 16, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 0}])\n",
    "    \n",
    "    # Layer  1\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=2, padding='same', name = 'max_1')(x)\n",
    "    \n",
    "    # Layer  2\n",
    "    x = _conv_block(x, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 2}])\n",
    "  \n",
    "    # Layer  3\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=2, padding='same', name = 'max_3')(x)\n",
    "    \n",
    "    # Layer  4\n",
    "    x = _conv_block(x, [{'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 4}])   \n",
    "\n",
    "    # Layer  5\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=2, padding='same', name = 'max_5')(x)\n",
    "    \n",
    "    # Layer  6\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 6}])   \n",
    "    \n",
    "    # Layer  7\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=2, padding='same', name = 'max_7')(x)\n",
    "    \n",
    "    # Layer  8\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 8}])   \n",
    "    layer_8 = x\n",
    "    \n",
    "    # Layer  9\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=2, padding='same', name = 'max_9')(x)\n",
    "    \n",
    "    # Layer  10\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 10}])\n",
    "    layer_10 = x\n",
    "    \n",
    "    # Layer  11\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=1, padding='same', name = 'max_11')(x)\n",
    "    \n",
    "    # Layer  12\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 12}])\n",
    "\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    # Layer  13\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 13}]) \n",
    "    layer_13 = x\n",
    "        \n",
    "    # Layer  14\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 14}])     \n",
    "    \n",
    "    # Layer  15\n",
    "    x = _conv_block(x, [{'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': True, 'activ': 0, 'layer_idx': 15}])    \n",
    "    \n",
    "    # Layer  16\n",
    "    yolo_16 = x\n",
    "    \n",
    "    \n",
    "    # Layer  17\n",
    "    x = layer_13\n",
    "\n",
    "    # Layer  18\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 18}])\n",
    "   \n",
    "    # Layer  19\n",
    "    x = UpSampling2D(size=(2, 2), name = 'upsamp_19')(x)\n",
    "\n",
    "    # Layer  20\n",
    "    x = concatenate([layer_8, x],  name='concatenate_20')\n",
    "    \n",
    "    # Layer  21\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'activ': 1, 'layer_idx': 21}])     \n",
    "    \n",
    "    # Layer  22\n",
    "    x = _conv_block(x, [{'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': True, 'activ': 0, 'layer_idx': 22}])    \n",
    "    \n",
    "    # Layer  23\n",
    "    yolo_23 = x   \n",
    "                                      \n",
    "    model = Model(input_image, [yolo_16, yolo_23], name = 'Yolo_v3_tiny')    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = make_yolov3_tiny_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import struct\n",
    "\n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,    = struct.unpack('i', w_f.read(4))\n",
    "            minor,    = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                print(\"reading 64 bytes\")\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                print(\"reading 32 bytes\")\n",
    "                w_f.read(4)\n",
    "\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            \n",
    "            binary = w_f.read()\n",
    "\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, model):\n",
    "        count = 0\n",
    "        ncount = 0\n",
    "        for i in range(23):\n",
    "            try:\n",
    "\n",
    "                conv_layer = model.get_layer('convn_' + str(i))\n",
    "                filter = conv_layer.kernel.shape[-1]\n",
    "                nweights = np.prod(conv_layer.kernel.shape) # kernel*kernel*c*filter\n",
    "                \n",
    "                print(\"loading weights of convolution #\" + str(i)+ \"- nb parameters: \"+str(nweights+filter))             \n",
    "                \n",
    "                if i  in [15, 22]:\n",
    "                    print(\"Special processing for layer \"+ str(i))\n",
    "                    bias  = self.read_bytes(filter) # bias\n",
    "                    weights = self.read_bytes(nweights) # weights\n",
    "                \n",
    "                else:                    \n",
    "                    bias  = self.read_bytes(filter) # bias\n",
    "                    scale = self.read_bytes(filter) # scale\n",
    "                    mean  = self.read_bytes(filter) # mean\n",
    "                    var   = self.read_bytes(filter) # variance\n",
    "                    weights = self.read_bytes(nweights) # weights\n",
    "                    \n",
    "                    bias = bias - scale  * mean / (np.sqrt(var + 0.00001)) #normalize bias\n",
    "\n",
    "                    weights = np.reshape(weights,(filter,int(nweights/filter)))  #normalize weights\n",
    "                    A = scale / (np.sqrt(var + 0.00001))\n",
    "                    A= np.expand_dims(A,axis=0)\n",
    "                    weights = weights* A.T\n",
    "                    weights = np.reshape(weights,(nweights))\n",
    "                \n",
    "\n",
    "                weights = weights.reshape(list(reversed(conv_layer.get_weights()[0].shape)))                 \n",
    "                weights = weights.transpose([2,3,1,0])\n",
    "                \n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    a=conv_layer.set_weights([weights, bias])\n",
    "                else:    \n",
    "                    a=conv_layer.set_weights([weights])\n",
    "                \n",
    "                count = count+1\n",
    "                ncount = ncount+nweights+filter\n",
    "             \n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i)) \n",
    "        \n",
    "        print(count, \"Conv normalized layers loaded \", ncount, \" parameters\")\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 64 bytes\n",
      "loading weights of convolution #0- nb parameters: 448\n",
      "no convolution #1\n",
      "loading weights of convolution #2- nb parameters: 4640\n",
      "no convolution #3\n",
      "loading weights of convolution #4- nb parameters: 18496\n",
      "no convolution #5\n",
      "loading weights of convolution #6- nb parameters: 73856\n",
      "no convolution #7\n",
      "loading weights of convolution #8- nb parameters: 295168\n",
      "no convolution #9\n",
      "loading weights of convolution #10- nb parameters: 1180160\n",
      "no convolution #11\n",
      "loading weights of convolution #12- nb parameters: 4719616\n",
      "loading weights of convolution #13- nb parameters: 262400\n",
      "loading weights of convolution #14- nb parameters: 1180160\n",
      "loading weights of convolution #15- nb parameters: 130815\n",
      "Special processing for layer 15\n",
      "no convolution #16\n",
      "no convolution #17\n",
      "loading weights of convolution #18- nb parameters: 32896\n",
      "no convolution #19\n",
      "no convolution #20\n",
      "loading weights of convolution #21- nb parameters: 884992\n",
      "loading weights of convolution #22- nb parameters: 65535\n",
      "Special processing for layer 22\n",
      "13 Conv normalized layers loaded  8849182  parameters\n"
     ]
    }
   ],
   "source": [
    "# Get and compute the weights\n",
    "weight_reader = WeightReader('models/yolo/yolov3-tiny.weights')\n",
    "weight_reader.load_weights(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Le volume dans le lecteur C s'appelle Windows\n",
      " Le num‚ro de s‚rie du volume est CCDC-AB26\n",
      "\n",
      " R‚pertoire de C:\\Users\\edh\\yolo4\\Yolov-4\\models\\yolo\n",
      "\n",
      "16/07/2020  08:35        35ÿ584ÿ360 yolov3-tiny.h5\n",
      "16/07/2020  08:26       258ÿ879ÿ904 yolov4.h5\n",
      "               2 fichier(s)      294ÿ464ÿ264 octets\n",
      "               0 R‚p(s)  292ÿ115ÿ058ÿ688 octets libres\n"
     ]
    }
   ],
   "source": [
    "# save the model to file\n",
    "! rm models\\yolo\\yolov3-tiny.h5\n",
    "\n",
    "model.save('models/yolo/yolov3-tiny.h5')\n",
    "! dir models\\yolo\\*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edh\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model, Model\n",
    "yolo_model = load_model(\"models/yolo/yolov3-tiny.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  input_0  trainable:  False\n",
      "layer  convn_0  trainable:  False\n",
      "layer  BN_0  trainable:  False\n",
      "layer  leaky_0  trainable:  False\n",
      "layer  max_1  trainable:  False\n",
      "layer  convn_2  trainable:  False\n",
      "layer  BN_2  trainable:  False\n",
      "layer  leaky_2  trainable:  False\n",
      "layer  max_3  trainable:  False\n",
      "layer  convn_4  trainable:  False\n",
      "layer  BN_4  trainable:  False\n",
      "layer  leaky_4  trainable:  False\n",
      "layer  max_5  trainable:  False\n",
      "layer  convn_6  trainable:  False\n",
      "layer  BN_6  trainable:  False\n",
      "layer  leaky_6  trainable:  False\n",
      "layer  max_7  trainable:  False\n",
      "layer  convn_8  trainable:  False\n",
      "layer  BN_8  trainable:  False\n",
      "layer  leaky_8  trainable:  False\n",
      "layer  max_9  trainable:  False\n",
      "layer  convn_10  trainable:  False\n",
      "layer  BN_10  trainable:  False\n",
      "layer  leaky_10  trainable:  False\n",
      "layer  max_11  trainable:  False\n",
      "layer  convn_12  trainable:  False\n",
      "layer  BN_12  trainable:  False\n",
      "layer  leaky_12  trainable:  False\n",
      "layer  convn_13  trainable:  True\n",
      "layer  BN_13  trainable:  True\n",
      "layer  leaky_13  trainable:  True\n",
      "layer  convn_18  trainable:  True\n",
      "layer  BN_18  trainable:  True\n",
      "layer  leaky_18  trainable:  True\n",
      "layer  upsamp_19  trainable:  True\n",
      "layer  concatenate_20  trainable:  True\n",
      "layer  convn_14  trainable:  True\n",
      "layer  convn_21  trainable:  True\n",
      "layer  BN_14  trainable:  True\n",
      "layer  BN_21  trainable:  True\n",
      "layer  leaky_14  trainable:  True\n",
      "layer  leaky_21  trainable:  True\n",
      "layer  convn_15  trainable:  True\n",
      "layer  convn_22  trainable:  True\n",
      "layer  BN_15  trainable:  True\n",
      "layer  BN_22  trainable:  True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the backbone\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = \"convn_13\"\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "train = False\n",
    "for l in yolo_model.layers:\n",
    "  if l.name == fine_tune_at:\n",
    "        train = True        \n",
    "  l.trainable =  train\n",
    "    \n",
    "\n",
    "# Display the trainable indicator\n",
    "for l in yolo_model.layers:\n",
    "    print(\"layer \",l.name, \" trainable: \", l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(labels_path):\n",
    "    with open(labels_path) as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [c.strip() for c in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "nb labels:  80\n"
     ]
    }
   ],
   "source": [
    "# Load the labels\n",
    "labels = read_labels(\"models/yolo/coco_classes.txt\")\n",
    "print(labels)\n",
    "print(\"nb labels: \",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_W        = 416\n",
    "NETWORK_H        = 416\n",
    "NB_BOX           = 3\n",
    "NB_CLASS         = len(labels)\n",
    "OBJ_THRESHOLD    = 0.3\n",
    "NMS_THRESHOLD    = 0.3\n",
    "grids = [(13,13), (26,26)]\n",
    "anchors = [[81,82,  135,169,  344,319], [10,14,  23,27,  37,58]]\n",
    "scales_x_y = [1.0, 1.0]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 32\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    "     \n",
    "        \n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "    \n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n",
    "\n",
    "\n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    " \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    " \n",
    "        return self.label\n",
    " \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    " \n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "from os.path import join\n",
    "\n",
    "def convert(image_wh, box, grid_w, grid_h, Boxanchor, yolo_id):\n",
    "    dw = image_wh[0]/ grid_w\n",
    "    dh = image_wh[1]/ grid_h\n",
    "    center_x = (box[0] + box[1])/2.0\n",
    "    center_x = center_x / dw\n",
    "    center_y = (box[2] + box[3])/2.0\n",
    "    center_y = center_y / dh\n",
    "    \n",
    "    grid_x = int(np.floor(center_x))\n",
    "    grid_y = int(np.floor(center_y))\n",
    "    \n",
    "    if grid_x < grid_w and grid_y < grid_h:\n",
    "        w = (box[1] - box[0]) / dw\n",
    "        h = (box[3] - box[2]) / dh\n",
    "        \n",
    "        # find the anchor that best predicts this box\n",
    "        best_anchor = -1\n",
    "        max_iou     = -1\n",
    "    \n",
    "        shifted_box = BoundBox(0,0,w,h)\n",
    "    \n",
    "        for i in range(len(anchors[yolo_id])//2):\n",
    "            iou    = bbox_iou(shifted_box, Boxanchor[i])                   \n",
    "            if max_iou < iou:\n",
    "                best_anchor = i\n",
    "                max_iou     = iou\n",
    "    \n",
    "        return (center_x,center_y,w,h,grid_x,grid_y,best_anchor)\n",
    "    \n",
    "    else: # not compatible with the grid size\n",
    "        return (0,0,0,0,0,0,-1)\n",
    "    \n",
    "\n",
    "def convert_annotation(year, image_set, image_id, grid_w, grid_h, Boxanchor, yolo_id, VOC_path):\n",
    "    in_file = open(VOC_path+'VOC%s\\\\Annotations\\\\%s.xml'%(year, image_id))\n",
    "    out_file = open('VOCYoloV3Tiny\\\\VOC%s_%s\\\\tiny_labels_%s\\\\%s.txt'%(year, image_set, yolo_id, image_id), 'w')\n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    image_w = int(size.find('width').text)\n",
    "    image_h = int(size.find('height').text)\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in labels or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = labels.index(cls)\n",
    "        \n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        bb = convert((image_w,image_h), b, grid_w, grid_h, Boxanchor, yolo_id)\n",
    "        \n",
    "        if bb[-1] != -1:\n",
    "            out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n",
    "\n",
    "\n",
    "def build_label_files (year, image_set, VOC_path):\n",
    "    yolo_id = 0\n",
    "    \n",
    "    for grid_w, grid_h in grids:\n",
    "        print(\"grid :\",grid_w, grid_h)\n",
    "\n",
    "        Boxanchor= [BoundBox(0, 0, anchors[yolo_id][2*i], anchors[yolo_id][2*i+1]) for i in range(int(len(anchors[yolo_id])//2))]\n",
    "       \n",
    "        if not os.path.exists('VOCYoloV3Tiny\\\\VOC%s_%s\\\\tiny_labels_%s\\\\' %(year, image_set, yolo_id)):\n",
    "            os.makedirs('VOCYoloV3Tiny\\\\VOC%s_%s\\\\tiny_labels_%s\\\\' %(year, image_set, yolo_id))\n",
    "        \n",
    "        image_ids = open(VOC_path+'VOC%s\\\\ImageSets\\\\Main\\\\%s.txt'%(year, image_set)).read().strip().split()\n",
    "\n",
    "        for image_id in image_ids:\n",
    "            convert_annotation(year, image_set, image_id, grid_w, grid_h, Boxanchor, yolo_id, VOC_path)\n",
    "            \n",
    "        yolo_id+=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid : 13 13\n",
      "grid : 26 26\n"
     ]
    }
   ],
   "source": [
    "# Build the label files for training\n",
    "VOC_path = '..\\\\train\\\\VOCdevkit\\\\'\n",
    "build_label_files ('2012', 'train', VOC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid : 13 13\n",
      "grid : 26 26\n"
     ]
    }
   ],
   "source": [
    "# Build the label files for validation\n",
    "build_label_files ('2012', 'val', VOC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, interpolation = 'bilinear', target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    \n",
    "    return image, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train (year, image_set, nb_train, VOC_path):\n",
    " \n",
    "    train_x  = np.zeros ((nb_train, NETWORK_H, NETWORK_W, 3), dtype=np.float32)\n",
    "    train_y0 = np.zeros ((nb_train, grids[0][1], grids[0][0], NB_BOX,(4+1+NB_CLASS)), dtype=np.float32)\n",
    "    train_y1 = np.zeros ((nb_train, grids[1][1], grids[1][0], NB_BOX,(4+1+NB_CLASS)), dtype=np.float32)\n",
    "    bc = 0 \n",
    "        \n",
    "    image_ids = open(VOC_path+'VOC%s\\\\ImageSets\\\\Main\\\\%s.txt'%(year, image_set)).read().strip().split()\n",
    "\n",
    "    \n",
    "    for image_id in image_ids:        \n",
    "        # Pre-process the image train_x\n",
    "        img_filename = VOC_path+'VOC%s\\\\JPEGImages\\\\%s.jpg'%(year, image_id)\n",
    "        image, image_w, image_h = load_image_pixels(img_filename, (NETWORK_W, NETWORK_H))\n",
    "        train_x[bc,:,:,:] = image\n",
    "        \n",
    "        # build true predict train_y0 and box b0\n",
    "        labels_file = open('VOCYoloV3Tiny\\\\VOC%s_%s\\\\tiny_labels_0\\\\%s.txt'%(year, image_set, image_id), 'r')\n",
    "        \n",
    "        rec = np.fromfile(labels_file, dtype=np.float32, sep = \" \")\n",
    "        for i in range(len(rec)//8):\n",
    "            classid,x,y,w,h,grid_x,grid_y,best_anchor = rec[8*i:8*(i+1)]\n",
    "            train_y0[bc, int(grid_y),int(grid_x),int(best_anchor), 0:4] = x,y,w,h\n",
    "            train_y0[bc, int(grid_y),int(grid_x),int(best_anchor), 4] = 1.\n",
    "            train_y0[bc, int(grid_y),int(grid_x),int(best_anchor), 5+ int(classid)] = 0.9 #Class label smoothing, use 0.9 instead of 1.0 in order to mitigate overfitting.\n",
    "            \n",
    "        # build true predict train_y1 and box b1\n",
    "        labels_file = open('VOCYoloV3Tiny\\\\VOC%s_%s\\\\tiny_labels_1\\\\%s.txt'%(year, image_set, image_id), 'r')\n",
    "        \n",
    "        rec = np.fromfile(labels_file, dtype=np.float32, sep = \" \")\n",
    "        true_box_index = 0\n",
    "        for i in range(len(rec)//8):\n",
    "            classid,x,y,w,h,grid_x,grid_y,best_anchor = rec[8*i:8*(i+1)]\n",
    "            train_y1[bc, int(grid_y),int(grid_x),int(best_anchor), 0:4] = x,y,w,h\n",
    "            train_y1[bc, int(grid_y),int(grid_x),int(best_anchor), 4] = 1.\n",
    "            train_y1[bc, int(grid_y),int(grid_x),int(best_anchor), 5+ int(classid)] = 0.9 #Class label smoothing, use 0.9 instead of 1.0 in order to mitigate overfitting.\n",
    "             \n",
    "        bc+=1\n",
    "        if bc == nb_train:\n",
    "            break\n",
    "     \n",
    "    train_y0 = np.reshape (train_y0, (nb_train, grids[0][1], grids[0][0], NB_BOX*(4+1+NB_CLASS)))\n",
    "    train_y1 = np.reshape (train_y1, (nb_train, grids[1][1], grids[1][0], NB_BOX*(4+1+NB_CLASS)))\n",
    "    \n",
    "    return(train_x,  [train_y0,train_y1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the data for training\n",
    "VOC_path = '..\\\\train\\\\VOCdevkit\\\\'\n",
    "nb_data_for_training= 640\n",
    "train_x, train_y = build_train('2012', 'train', (nb_data_for_training//BATCH_SIZE)*BATCH_SIZE, VOC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the data for validation\n",
    "nb_data_for_validation= 320\n",
    "val_x, val_y = build_train('2012', 'val', (nb_data_for_validation//BATCH_SIZE)*BATCH_SIZE, VOC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "   \n",
    "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "    print(\"Keras version: {}\".format(tf.keras.__version__))\n",
    "   \n",
    "    print(y_pred.shape)\n",
    "    grid_h, grid_w = y_pred.shape[1:3] \n",
    "    print (\"grid_h, grid_w\",grid_h, grid_w)\n",
    "    \n",
    "    if grid_h == 13:\n",
    "        anchor = anchors[0]\n",
    "    else:    \n",
    "        anchor = anchors[1]     \n",
    "        \n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    print (\"mask_shape\",mask_shape)\n",
    "\n",
    "    \n",
    "    cell_x = tf.cast((tf.reshape(tf.tile(tf.range(grid_w), [grid_h]), (1, grid_h, grid_w, 1, 1))),dtype=tf.float32)\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, NB_BOX, 1])\n",
    "    \n",
    "    \n",
    "    ######  prediction\n",
    "    y_pred = tf.reshape(y_pred, (BATCH_SIZE, grid_h, grid_w, NB_BOX, NB_CLASS+5))\n",
    "\n",
    "    ### adjust x and y  \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) # x, y)\n",
    "    pred_box_xy = pred_box_xy + cell_grid\n",
    "    \n",
    "     ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(anchor, [1,1,1,NB_BOX,2]) / np.full((1,1,1,NB_BOX, 2), [NETWORK_W, NETWORK_H])\n",
    "  \n",
    "    ### adjust objectness\n",
    "    pred_box_obj = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = tf.sigmoid(y_pred[..., 5:])\n",
    "    \n",
    "    \n",
    "    ######  true\n",
    "    y_true = tf.reshape(y_true, (BATCH_SIZE, grid_h, grid_w, NB_BOX, NB_CLASS+5))\n",
    "    print (\"y_true\", y_true.shape)\n",
    "\n",
    "    ### adjust x and y  \n",
    "    true_box_xy = y_true[..., :2] # x, y\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4]\n",
    "    \n",
    "    ### adjust objectness\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "\n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "\n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    \n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas + 1e-10\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "   \n",
    "    true_box_obj = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "\n",
    "    \n",
    "    \n",
    "    ######  coefficients   \n",
    "   \n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    ### is 1 when there is an object in the cell i, else 0.\n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "    print(\"sum coord_mask\",tf.reduce_sum(coord_mask))\n",
    "    \n",
    "    ### objectness mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    for i in range(BATCH_SIZE):\n",
    "        bd = y_true[i,:,:,:,:4]\n",
    "        nozero = tf.not_equal(bd, tf.zeros((grid_h, grid_w, NB_BOX, 4)))\n",
    "        bdd = tf.boolean_mask(bd, nozero)\n",
    "        s=tf.squeeze(tf.size(bdd)//4)\n",
    "        c= tf.zeros((50-s,4))\n",
    "        bdd=tf.reshape(bdd, (s,4))\n",
    "        bdd = tf.concat([bdd,c],axis=0)\n",
    "        bdd = tf.expand_dims(bdd,0)\n",
    "        bdd = tf.expand_dims(bdd,0)\n",
    "        bdd = tf.expand_dims(bdd,0)\n",
    "        bdd = tf.expand_dims(bdd,0)\n",
    "        if (i==0):\n",
    "            true_boxes =bdd\n",
    "        else:\n",
    "            true_boxes = tf.concat([true_boxes,bdd], axis=0)  \n",
    "    \n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    print(\"best_ious\", tf.reduce_max(best_ious))\n",
    "    \n",
    "    obj_mask = tf.zeros(mask_shape)\n",
    "    obj_mask = tf.cast((best_ious < 0.6),dtype=tf.float32) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    obj_mask = obj_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "\n",
    "    print(\"sum obj_mask\",tf.reduce_sum(obj_mask))\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    ### is 1 when there is a particular class is predicted, else 0.\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    class_weights = np.ones(NB_CLASS, dtype='float32')\n",
    "    class_mask = y_true[..., 4] * tf.gather(class_weights, true_box_class) * CLASS_SCALE\n",
    "    print(\"sum class_mask\",tf.reduce_sum(class_mask))\n",
    "    \n",
    "    nb_coord_box = tf.reduce_sum(tf.cast((coord_mask > 0.0),dtype=tf.float32))\n",
    "    nb_obj_box  = tf.reduce_sum(tf.cast((obj_mask  > 0.0),dtype=tf.float32))\n",
    "    nb_class_box = tf.reduce_sum(tf.cast((class_mask > 0.0),dtype=tf.float32))\n",
    "    print(\"nb_coord_box\",nb_coord_box)\n",
    "    print(\"nb_obj_box\",nb_obj_box)\n",
    "    print(\"nb_class_box\",nb_class_box) \n",
    "      \n",
    "    ### loss\n",
    "    loss_xy    = tf.reduce_sum(coord_mask * tf.square(true_box_xy - pred_box_xy)) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(coord_mask * tf.square(tf.sqrt(tf.abs(true_box_wh)) - tf.sqrt(tf.abs(pred_box_wh)))) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_obj   = tf.reduce_sum(obj_mask * tf.square(true_box_obj-pred_box_obj)) / (nb_obj_box + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(class_mask * loss_class) / (nb_class_box + 1e-6)\n",
    "\n",
    "    print(\"loss_xy\",loss_xy.shape)\n",
    "    print(\"sum loss_xy\",tf.reduce_sum(loss_xy))\n",
    "    \n",
    "    print(\"loss_wh\",loss_wh.shape)\n",
    "    print(\"sum loss_wh\",tf.reduce_sum(loss_wh))\n",
    "    \n",
    "    print(\"loss_obj\",loss_obj.shape)\n",
    "    print(\"sum loss_obj\",tf.reduce_sum(loss_obj))\n",
    "        \n",
    "    print(\"loss_class\",loss_class.shape)\n",
    "    print(\"sum loss_class\",tf.reduce_sum(loss_class))\n",
    "    \n",
    "    loss = loss_xy + loss_wh + loss_obj + loss_class\n",
    "    print(\"loss\",loss.shape)\n",
    "    print()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "#optimizer = tf.keras.optimizers.SGD.RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0\n",
      "Eager execution: False\n",
      "Keras version: 2.2.4-tf\n",
      "(None, 13, 13, 255)\n",
      "grid_h, grid_w 13 13\n",
      "mask_shape Tensor(\"loss/BN_15_loss/custom_loss/strided_slice:0\", shape=(4,), dtype=int32)\n",
      "y_true (32, 13, 13, 3, 85)\n",
      "sum coord_mask Tensor(\"loss/BN_15_loss/custom_loss/Sum:0\", shape=(), dtype=float32)\n",
      "best_ious Tensor(\"loss/BN_15_loss/custom_loss/Max_1:0\", shape=(), dtype=float32)\n",
      "sum obj_mask Tensor(\"loss/BN_15_loss/custom_loss/Sum_1:0\", shape=(), dtype=float32)\n",
      "sum class_mask Tensor(\"loss/BN_15_loss/custom_loss/Sum_2:0\", shape=(), dtype=float32)\n",
      "nb_coord_box Tensor(\"loss/BN_15_loss/custom_loss/Sum_3:0\", shape=(), dtype=float32)\n",
      "nb_obj_box Tensor(\"loss/BN_15_loss/custom_loss/Sum_4:0\", shape=(), dtype=float32)\n",
      "nb_class_box Tensor(\"loss/BN_15_loss/custom_loss/Sum_5:0\", shape=(), dtype=float32)\n",
      "loss_xy ()\n",
      "sum loss_xy Tensor(\"loss/BN_15_loss/custom_loss/Sum_10:0\", shape=(), dtype=float32)\n",
      "loss_wh ()\n",
      "sum loss_wh Tensor(\"loss/BN_15_loss/custom_loss/Sum_11:0\", shape=(), dtype=float32)\n",
      "loss_obj ()\n",
      "sum loss_obj Tensor(\"loss/BN_15_loss/custom_loss/Sum_12:0\", shape=(), dtype=float32)\n",
      "loss_class ()\n",
      "sum loss_class Tensor(\"loss/BN_15_loss/custom_loss/Sum_13:0\", shape=(), dtype=float32)\n",
      "loss ()\n",
      "\n",
      "TensorFlow version: 2.1.0\n",
      "Eager execution: False\n",
      "Keras version: 2.2.4-tf\n",
      "(None, 26, 26, 255)\n",
      "grid_h, grid_w 26 26\n",
      "mask_shape Tensor(\"loss/BN_22_loss/custom_loss/strided_slice:0\", shape=(4,), dtype=int32)\n",
      "y_true (32, 26, 26, 3, 85)\n",
      "sum coord_mask Tensor(\"loss/BN_22_loss/custom_loss/Sum:0\", shape=(), dtype=float32)\n",
      "best_ious Tensor(\"loss/BN_22_loss/custom_loss/Max_1:0\", shape=(), dtype=float32)\n",
      "sum obj_mask Tensor(\"loss/BN_22_loss/custom_loss/Sum_1:0\", shape=(), dtype=float32)\n",
      "sum class_mask Tensor(\"loss/BN_22_loss/custom_loss/Sum_2:0\", shape=(), dtype=float32)\n",
      "nb_coord_box Tensor(\"loss/BN_22_loss/custom_loss/Sum_3:0\", shape=(), dtype=float32)\n",
      "nb_obj_box Tensor(\"loss/BN_22_loss/custom_loss/Sum_4:0\", shape=(), dtype=float32)\n",
      "nb_class_box Tensor(\"loss/BN_22_loss/custom_loss/Sum_5:0\", shape=(), dtype=float32)\n",
      "loss_xy ()\n",
      "sum loss_xy Tensor(\"loss/BN_22_loss/custom_loss/Sum_10:0\", shape=(), dtype=float32)\n",
      "loss_wh ()\n",
      "sum loss_wh Tensor(\"loss/BN_22_loss/custom_loss/Sum_11:0\", shape=(), dtype=float32)\n",
      "loss_obj ()\n",
      "sum loss_obj Tensor(\"loss/BN_22_loss/custom_loss/Sum_12:0\", shape=(), dtype=float32)\n",
      "loss_class ()\n",
      "sum loss_class Tensor(\"loss/BN_22_loss/custom_loss/Sum_13:0\", shape=(), dtype=float32)\n",
      "loss ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model using the custom loss function defined above\n",
    "yolo_model.compile(loss=custom_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Starting train\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Stop train\")\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(\"--Start epoch {}\".format(epoch))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"--End epoch {}, the average training loss is {:7.2f}, testing loss is {:7.2f}\".format(epoch, logs[\"loss\"], logs[\"val_loss\"]))        \n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        print(\"---Start training batch {}, size {}\".format(batch,logs[\"size\"]))\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"---End training batch {}, total loss is {:7.2f}, loss (13*13) is {:7.2f}, loss (26*26) is {:7.2f}\"\n",
    "              .format(batch, logs[\"loss\"],logs[\"BN_15_loss\"],logs[\"BN_22_loss\"]))      \n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        print(\"-Start testing\")\n",
    "        \n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"-Stop testing\")\n",
    "    \n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        print(\"---Start testing batch {}, size {}\".format(batch,logs[\"size\"]))\n",
    "        \n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"---End testing batch {}, total loss is {:7.2f}, loss (13*13) is {:7.2f}, loss (26*26) is {:7.2f}\"\n",
    "              .format(batch, logs[\"loss\"],logs[\"BN_15_loss\"],logs[\"BN_22_loss\"]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 320 samples\n",
      "Starting train\n",
      "Epoch 1/10\n",
      "--Start epoch 0\n",
      "---Start training batch 0, size 32\n",
      "---End training batch 0, total loss is   20.29, loss (13*13) is    7.21, loss (26*26) is   13.08\n",
      "---Start training batch 1, size 32\n",
      "---End training batch 1, total loss is   20.84, loss (13*13) is    7.10, loss (26*26) is   13.46\n",
      "---Start training batch 2, size 32\n",
      "---End training batch 2, total loss is   18.44, loss (13*13) is    6.79, loss (26*26) is   13.07\n",
      "---Start training batch 3, size 32\n",
      "---End training batch 3, total loss is   21.16, loss (13*13) is    6.85, loss (26*26) is   13.33\n",
      "---Start training batch 4, size 32\n",
      "---End training batch 4, total loss is   18.96, loss (13*13) is    6.74, loss (26*26) is   13.19\n",
      "---Start training batch 5, size 32\n",
      "---End training batch 5, total loss is   18.39, loss (13*13) is    6.66, loss (26*26) is   13.02\n",
      "---Start training batch 6, size 32\n",
      "---End training batch 6, total loss is   19.09, loss (13*13) is    6.63, loss (26*26) is   12.97\n",
      "---Start training batch 7, size 32\n",
      "---End training batch 7, total loss is   20.37, loss (13*13) is    6.59, loss (26*26) is   13.10\n",
      "---Start training batch 8, size 32\n",
      "---End training batch 8, total loss is   17.55, loss (13*13) is    6.52, loss (26*26) is   12.93\n",
      "---Start training batch 9, size 32\n",
      "---End training batch 9, total loss is   19.79, loss (13*13) is    6.53, loss (26*26) is   12.95\n",
      "---Start training batch 10, size 32\n",
      "---End training batch 10, total loss is   18.20, loss (13*13) is    6.49, loss (26*26) is   12.88\n",
      "---Start training batch 11, size 32\n",
      "---End training batch 11, total loss is   19.83, loss (13*13) is    6.48, loss (26*26) is   12.93\n",
      "---Start training batch 12, size 32\n",
      "---End training batch 12, total loss is   18.45, loss (13*13) is    6.44, loss (26*26) is   12.89\n",
      "---Start training batch 13, size 32\n",
      "---End training batch 13, total loss is   16.49, loss (13*13) is    6.38, loss (26*26) is   12.75\n",
      "---Start training batch 14, size 32\n",
      "---End training batch 14, total loss is   17.11, loss (13*13) is    6.35, loss (26*26) is   12.64\n",
      "---Start training batch 15, size 32\n",
      "---End training batch 15, total loss is   16.42, loss (13*13) is    6.32, loss (26*26) is   12.51\n",
      "---Start training batch 16, size 32\n",
      "---End training batch 16, total loss is   16.63, loss (13*13) is    6.28, loss (26*26) is   12.42\n",
      "---Start training batch 17, size 32\n",
      "---End training batch 17, total loss is   18.47, loss (13*13) is    6.27, loss (26*26) is   12.42\n",
      "---Start training batch 18, size 32\n",
      "---End training batch 18, total loss is   18.21, loss (13*13) is    6.26, loss (26*26) is   12.40\n",
      "---Start training batch 19, size 32\n",
      "---End training batch 19, total loss is   19.22, loss (13*13) is    6.26, loss (26*26) is   12.43\n",
      "-Start testing\n",
      "---Start testing batch 0, size 32\n",
      "---End testing batch 0, total loss is 19682950.00, loss (13*13) is  592.61, loss (26*26) is 19682358.00\n",
      "---Start testing batch 1, size 32\n",
      "---End testing batch 1, total loss is 1792769536.00, loss (13*13) is  684.68, loss (26*26) is 906225536.00\n",
      "---Start testing batch 2, size 32\n",
      "---End testing batch 2, total loss is 2215180032.00, loss (13*13) is 1035.40, loss (26*26) is 1342543104.00\n",
      "---Start testing batch 3, size 32\n",
      "---End testing batch 3, total loss is 2113479168.00, loss (13*13) is 1162.45, loss (26*26) is 1535276800.00\n",
      "---Start testing batch 4, size 32\n",
      "---End testing batch 4, total loss is 1768510720.00, loss (13*13) is 1004.66, loss (26*26) is 1581923584.00\n",
      "---Start testing batch 5, size 32\n",
      "---End testing batch 5, total loss is 7088366592.00, loss (13*13) is 1639.82, loss (26*26) is 2499663104.00\n",
      "---Start testing batch 6, size 32\n",
      "---End testing batch 6, total loss is 57117784.00, loss (13*13) is 1576.66, loss (26*26) is 2150727936.00\n",
      "---Start testing batch 7, size 32\n",
      "---End testing batch 7, total loss is 6510075392.00, loss (13*13) is 2193.62, loss (26*26) is 2695645696.00\n",
      "---Start testing batch 8, size 32\n",
      "---End testing batch 8, total loss is 24277946368.00, loss (13*13) is 2308.25, loss (26*26) is 5093678592.00\n",
      "---Start testing batch 9, size 32\n",
      "---End testing batch 9, total loss is 372803872.00, loss (13*13) is 2158.56, loss (26*26) is 4621591040.00\n",
      "-Stop testing\n",
      " - 217s - loss: 18.6951 - BN_15_loss: 6.2621 - BN_22_loss: 12.4330 - val_loss: 4621593241.4000 - val_BN_15_loss: 2158.5562 - val_BN_22_loss: 4621591040.0000\n",
      "--End epoch 0, the average training loss is   18.70, testing loss is 4621593241.40\n",
      "Epoch 2/10\n",
      "--Start epoch 1\n",
      "---Start training batch 0, size 32\n",
      "---End training batch 0, total loss is   15.63, loss (13*13) is    5.47, loss (26*26) is   10.16\n",
      "---Start training batch 1, size 32\n",
      "---End training batch 1, total loss is   16.68, loss (13*13) is    5.56, loss (26*26) is   10.60\n",
      "---Start training batch 2, size 32\n",
      "---End training batch 2, total loss is   15.93, loss (13*13) is    5.56, loss (26*26) is   10.52\n",
      "---Start training batch 3, size 32\n",
      "---End training batch 3, total loss is   16.16, loss (13*13) is    5.51, loss (26*26) is   10.59\n",
      "---Start training batch 4, size 32\n",
      "---End training batch 4, total loss is   16.14, loss (13*13) is    5.51, loss (26*26) is   10.60\n",
      "---Start training batch 5, size 32\n",
      "---End training batch 5, total loss is   17.61, loss (13*13) is    5.52, loss (26*26) is   10.84\n",
      "---Start training batch 6, size 32\n",
      "---End training batch 6, total loss is   18.58, loss (13*13) is    5.59, loss (26*26) is   11.08\n",
      "---Start training batch 7, size 32\n",
      "---End training batch 7, total loss is   18.87, loss (13*13) is    5.66, loss (26*26) is   11.29\n",
      "---Start training batch 8, size 32\n",
      "---End training batch 8, total loss is   16.33, loss (13*13) is    5.65, loss (26*26) is   11.23\n",
      "---Start training batch 9, size 32\n",
      "---End training batch 9, total loss is   15.86, loss (13*13) is    5.63, loss (26*26) is   11.15\n",
      "---Start training batch 10, size 32\n",
      "---End training batch 10, total loss is   15.53, loss (13*13) is    5.62, loss (26*26) is   11.05\n",
      "---Start training batch 11, size 32\n",
      "---End training batch 11, total loss is   14.96, loss (13*13) is    5.60, loss (26*26) is   10.93\n",
      "---Start training batch 12, size 32\n",
      "---End training batch 12, total loss is   16.86, loss (13*13) is    5.60, loss (26*26) is   10.95\n",
      "---Start training batch 13, size 32\n",
      "---End training batch 13, total loss is   15.30, loss (13*13) is    5.59, loss (26*26) is   10.87\n",
      "---Start training batch 14, size 32\n",
      "---End training batch 14, total loss is   16.72, loss (13*13) is    5.59, loss (26*26) is   10.89\n",
      "---Start training batch 15, size 32\n",
      "---End training batch 15, total loss is   17.64, loss (13*13) is    5.61, loss (26*26) is   10.94\n",
      "---Start training batch 16, size 32\n",
      "---End training batch 16, total loss is   16.79, loss (13*13) is    5.61, loss (26*26) is   10.96\n",
      "---Start training batch 17, size 32\n",
      "---End training batch 17, total loss is   16.65, loss (13*13) is    5.62, loss (26*26) is   10.95\n",
      "---Start training batch 18, size 32\n",
      "---End training batch 18, total loss is   16.49, loss (13*13) is    5.62, loss (26*26) is   10.94\n",
      "---Start training batch 19, size 32\n",
      "---End training batch 19, total loss is   15.39, loss (13*13) is    5.61, loss (26*26) is   10.89\n",
      "-Start testing\n",
      "---Start testing batch 0, size 32\n",
      "---End testing batch 0, total loss is 284595716096.00, loss (13*13) is  855.44, loss (26*26) is 284595716096.00\n",
      "---Start testing batch 1, size 32\n",
      "---End testing batch 1, total loss is 210133405663232.00, loss (13*13) is 2252.00, loss (26*26) is 105208998789120.00\n",
      "---Start testing batch 2, size 32\n",
      "---End testing batch 2, total loss is 87071270696386560.00, loss (13*13) is 2125.80, loss (26*26) is 29093895862222848.00\n",
      "---Start testing batch 3, size 32\n",
      "---End testing batch 3, total loss is 8376197691998208.00, loss (13*13) is 2384.42, loss (26*26) is 23914472393408512.00\n",
      "---Start testing batch 4, size 32\n",
      "---End testing batch 4, total loss is 114291520831488.00, loss (13*13) is 2060.97, loss (26*26) is 19154435301179392.00\n",
      "---Start testing batch 5, size 32\n",
      "---End testing batch 5, total loss is 4384728891260928.00, loss (13*13) is 3417.44, loss (26*26) is 16692818818891776.00\n",
      "---Start testing batch 6, size 32\n",
      "---End testing batch 6, total loss is 2093612275859456.00, loss (13*13) is 3299.79, loss (26*26) is 14607218133434368.00\n",
      "---Start testing batch 7, size 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---End testing batch 7, total loss is 1228596904217018368.00, loss (13*13) is 12378.61, loss (26*26) is 166355937483816960.00\n",
      "---Start testing batch 8, size 32\n",
      "---End testing batch 8, total loss is 140317191445676032.00, loss (13*13) is 12623.65, loss (26*26) is 163462744434016256.00\n",
      "---Start testing batch 9, size 32\n",
      "---End testing batch 9, total loss is 7565008829415424.00, loss (13*13) is 11697.63, loss (26*26) is 147872975222210560.00\n",
      "-Stop testing\n",
      " - 201s - loss: 16.5057 - BN_15_loss: 5.6129 - BN_22_loss: 10.8928 - val_loss: 147872962356982592.0000 - val_BN_15_loss: 11697.6348 - val_BN_22_loss: 147872975222210560.0000\n",
      "--End epoch 1, the average training loss is   16.51, testing loss is 147872962356982592.00\n",
      "Epoch 3/10\n",
      "--Start epoch 2\n",
      "---Start training batch 0, size 32\n",
      "---End training batch 0, total loss is   18.25, loss (13*13) is    6.12, loss (26*26) is   12.13\n",
      "---Start training batch 1, size 32\n",
      "---End training batch 1, total loss is   16.66, loss (13*13) is    5.97, loss (26*26) is   11.49\n",
      "---Start training batch 2, size 32\n",
      "---End training batch 2, total loss is   13.98, loss (13*13) is    5.71, loss (26*26) is   10.59\n",
      "---Start training batch 3, size 32\n",
      "---End training batch 3, total loss is   16.31, loss (13*13) is    5.68, loss (26*26) is   10.61\n",
      "---Start training batch 4, size 32\n",
      "---End training batch 4, total loss is   15.63, loss (13*13) is    5.60, loss (26*26) is   10.57\n",
      "---Start training batch 5, size 32\n",
      "---End training batch 5, total loss is   15.58, loss (13*13) is    5.55, loss (26*26) is   10.52\n",
      "---Start training batch 6, size 32\n",
      "---End training batch 6, total loss is   15.55, loss (13*13) is    5.53, loss (26*26) is   10.46\n",
      "---Start training batch 7, size 32\n",
      "---End training batch 7, total loss is   16.81, loss (13*13) is    5.54, loss (26*26) is   10.56\n",
      "---Start training batch 8, size 32\n",
      "---End training batch 8, total loss is   15.62, loss (13*13) is    5.51, loss (26*26) is   10.54\n",
      "---Start training batch 9, size 32\n",
      "---End training batch 9, total loss is   14.08, loss (13*13) is    5.45, loss (26*26) is   10.40\n",
      "---Start training batch 10, size 32\n",
      "---End training batch 10, total loss is   15.95, loss (13*13) is    5.45, loss (26*26) is   10.41\n",
      "---Start training batch 11, size 32\n",
      "---End training batch 11, total loss is   14.34, loss (13*13) is    5.43, loss (26*26) is   10.30\n",
      "---Start training batch 12, size 32\n",
      "---End training batch 12, total loss is   15.72, loss (13*13) is    5.43, loss (26*26) is   10.30\n",
      "---Start training batch 13, size 32\n",
      "---End training batch 13, total loss is   15.70, loss (13*13) is    5.43, loss (26*26) is   10.30\n",
      "---Start training batch 14, size 32\n",
      "---End training batch 14, total loss is   15.51, loss (13*13) is    5.41, loss (26*26) is   10.30\n",
      "---Start training batch 15, size 32\n",
      "---End training batch 15, total loss is   15.22, loss (13*13) is    5.41, loss (26*26) is   10.27\n",
      "---Start training batch 16, size 32\n",
      "---End training batch 16, total loss is   14.35, loss (13*13) is    5.39, loss (26*26) is   10.22\n",
      "---Start training batch 17, size 32\n",
      "---End training batch 17, total loss is   18.48, loss (13*13) is    5.41, loss (26*26) is   10.35\n",
      "---Start training batch 18, size 32\n",
      "---End training batch 18, total loss is   15.36, loss (13*13) is    5.41, loss (26*26) is   10.34\n",
      "---Start training batch 19, size 32\n",
      "---End training batch 19, total loss is   15.26, loss (13*13) is    5.39, loss (26*26) is   10.33\n",
      "-Start testing\n",
      "---Start testing batch 0, size 32\n",
      "---End testing batch 0, total loss is 3861732352.00, loss (13*13) is  194.93, loss (26*26) is 3861732096.00\n",
      "---Start testing batch 1, size 32\n",
      "---End testing batch 1, total loss is 888925388800.00, loss (13*13) is  481.95, loss (26*26) is 446393548800.00\n",
      "---Start testing batch 2, size 32\n",
      "---End testing batch 2, total loss is 358115413327872.00, loss (13*13) is  411.53, loss (26*26) is 119669398700032.00\n",
      "---Start testing batch 3, size 32\n",
      "---End testing batch 3, total loss is 12345263943319552.00, loss (13*13) is  378.24, loss (26*26) is 3176067917414400.00\n",
      "---Start testing batch 4, size 32\n",
      "---End testing batch 4, total loss is 33921527971840.00, loss (13*13) is  340.37, loss (26*26) is 2547638771646464.00\n",
      "---Start testing batch 5, size 32\n",
      "---End testing batch 5, total loss is 33084076457984.00, loss (13*13) is  422.28, loss (26*26) is 2128546197667840.00\n",
      "---Start testing batch 6, size 32\n",
      "---End testing batch 6, total loss is 5276873055535104.00, loss (13*13) is  434.60, loss (26*26) is 2578307254059008.00\n",
      "---Start testing batch 7, size 32\n",
      "---End testing batch 7, total loss is 39635149389824.00, loss (13*13) is  445.95, loss (26*26) is 2260973326958592.00\n",
      "---Start testing batch 8, size 32\n",
      "---End testing batch 8, total loss is 15103974965248.00, loss (13*13) is  653.09, loss (26*26) is 2011432237400064.00\n",
      "---Start testing batch 9, size 32\n",
      "---End testing batch 9, total loss is 61464207425536.00, loss (13*13) is  625.07, loss (26*26) is 1816435487670272.00\n",
      "-Stop testing\n",
      " - 139s - loss: 15.7179 - BN_15_loss: 5.3889 - BN_22_loss: 10.3289 - val_loss: 1816435413551411.2500 - val_BN_15_loss: 625.0710 - val_BN_22_loss: 1816435487670272.0000\n",
      "--End epoch 2, the average training loss is   15.72, testing loss is 1816435413551411.25\n",
      "Stop train\n",
      "elapsed seconds:  573\n"
     ]
    }
   ],
   "source": [
    "# Fit the model including validation data\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "EScallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "history = yolo_model.fit(x=train_x, y=train_y, validation_data = (val_x,val_y), epochs= 10,batch_size =BATCH_SIZE, verbose=2, callbacks=[CustomCallback(),EScallback])\n",
    "\n",
    "elapsed = datetime.datetime.now()-start\n",
    "print(\"elapsed seconds: \",elapsed.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves\n",
    "from matplotlib import pyplot\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
